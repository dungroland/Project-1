import torch
from transformers import DistilBertTokenizer, DistilBertForSequenceClassification
import re

class BERTSentimentPredictor:
    def __init__(self, model_path="models/distilbert"):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"S·ª≠ d·ª•ng device: {self.device}")
        
        # T·∫£i tokenizer v√† model
        self.tokenizer = DistilBertTokenizer.from_pretrained(model_path)
        self.model = DistilBertForSequenceClassification.from_pretrained(model_path)
        self.model.to(self.device)
        self.model.eval()
        
        # Mapping nh√£n
        self.label_map = {0: "Ti√™u c·ª±c (Negative)", 1: "T√≠ch c·ª±c (Positive)"}
    
    def preprocess_text(self, text):
        """Ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n (t∆∞∆°ng t·ª± nh∆∞ l√∫c training)"""
        text = text.lower()
        text = re.sub(r"<.*?>", " ", text)  # Lo·∫°i b·ªè HTML
        text = re.sub(r"[^a-zA-Z\s]", " ", text)  # Lo·∫°i b·ªè k√Ω t·ª± ƒë·∫∑c bi·ªát
        text = re.sub(r"\s+", " ", text).strip()  # Lo·∫°i b·ªè kho·∫£ng tr·∫Øng th·ª´a
        return text
    
    def predict(self, text):
        """D·ª± ƒëo√°n c·∫£m x√∫c cho m·ªôt vƒÉn b·∫£n"""
        # Ti·ªÅn x·ª≠ l√Ω
        clean_text = self.preprocess_text(text)
        
        # Tokenize
        inputs = self.tokenizer(
            clean_text,
            return_tensors="pt",
            truncation=True,
            padding=True,
            max_length=512
        )
        
        # Chuy·ªÉn sang device
        inputs = {key: value.to(self.device) for key, value in inputs.items()}
        
        # D·ª± ƒëo√°n
        with torch.no_grad():
            outputs = self.model(**inputs)
            predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)
            predicted_class = torch.argmax(predictions, dim=-1).item()
            confidence = predictions[0][predicted_class].item()
        
        return {
            'prediction': self.label_map[predicted_class],
            'confidence': confidence,
            'probabilities': {
                'negative': predictions[0][0].item(),
                'positive': predictions[0][1].item()
            }
        }
    
    def predict_batch(self, texts):
        """D·ª± ƒëo√°n cho nhi·ªÅu vƒÉn b·∫£n c√πng l√∫c"""
        results = []
        for text in texts:
            result = self.predict(text)
            results.append(result)
        return results

def main():
    """Demo s·ª≠ d·ª•ng BERT predictor"""
    try:
        # Kh·ªüi t·∫°o predictor
        predictor = BERTSentimentPredictor()
        
        print("=== BERT Sentiment Analysis Demo ===")
        print("Nh·∫≠p 'exit' ƒë·ªÉ tho√°t\n")
        
        while True:
            user_input = input("Nh·∫≠p ƒë√°nh gi√° phim: ")
            
            if user_input.lower() == 'exit':
                break
            
            if user_input.strip() == "":
                print("Vui l√≤ng nh·∫≠p n·ªôi dung!")
                continue
            
            # D·ª± ƒëo√°n
            result = predictor.predict(user_input)
            
            # Hi·ªÉn th·ªã k·∫øt qu·∫£
            print(f"\nüéØ K·∫øt qu·∫£: {result['prediction']}")
            print(f"üìä ƒê·ªô tin c·∫≠y: {result['confidence']:.2%}")
            print(f"üìà Chi ti·∫øt:")
            print(f"   ‚Ä¢ Negative: {result['probabilities']['negative']:.2%}")
            print(f"   ‚Ä¢ Positive: {result['probabilities']['positive']:.2%}")
            print("-" * 50)
    
    except FileNotFoundError:
        print("‚ùå Kh√¥ng t√¨m th·∫•y m√¥ h√¨nh BERT!")
        print("Vui l√≤ng ch·∫°y 'python src/train_bert_model.py' tr∆∞·ªõc.")
    except Exception as e:
        print(f"‚ùå L·ªói: {e}")

if __name__ == "__main__":
    main()